BibliographyType,ISBN,Identifier,Author,Title,Journal,Volume,Number,Month,Pages,Year,Address,Note,URL,Booktitle,Chapter,Edition,Series,Editor,Publisher,ReportType,Howpublished,Institution,Organizations,School,Annote,Custom1,Custom2,Custom3,Custom4,Custom5
10,"","","Moore, Gordon Earle","Cramming more components onto integrated circuits","",,,"4","",1965,"","","https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf","","","","","Semiconductor, Fairchild","","","","","","","","With unit cost falling as the number of components percircuit rises, by 1975 economics may dictate squeezing asmany as 65,000 components on a single silicon chip.","","moore's law, integrated circuit, fairchild semiconductor","",""
7,"","","Sugarman, Robert","Does the country need a good $20 microprocessor?","The Engineering Newspaper for the Electronics Industry",,,"8","",1975,"","","https://www.commodore.ca/gallery/magazines/misc/mos_605x_team_eetimes_august_1975.pdf","","","","","","","","","","","","","MOS Technology MCS650X microprocessor designers gather around a 200X print of the CPU Rubylith, color-coded for debugging into metallization, polysilicon and diffusion layers. In the background is a 1000X expansion of the internal 21X143 decode-ROM, which manager Chuck Peddle claims is a key factor in obtaining small chip size.","","6502, rubylith, engineering, chuck peddle","",""
10,"","","Moore, Gordon Earle","Progress in digital integrated electronics","",,,"","",1975,"","","https://www.eng.auburn.edu/~agrawvd/COURSE/E7770_Spr07/READ/Gordon_Moore_1975_Speech.pdf","","","","","Intel","","","","","","","","","","","",""
1,"","","Waite, Mitchell","Computer Graphics Primer","",,,"","",1979,"","","https://www.atariarchives.org/cgp/Ch02_Sec25.php","","","","","Howard W. Sams \& Co., Inc.","","","","","","","","Perhaps no single technology has had more impact on people than television. Yet according to the experts the real impact is just starting.  The reason? Home computers that connect to a standard television and convert it into a machine with more raw power than any product ever offered to the consumer and with the capability to completely alter the way we relate to the visual world of electronics.  This book is about one of the most exciting uses of the new home computer products—computer graphics—the ability to create complex drawings, plans, maps, and schematics on the screen of an ordinary black-and-white or color television. It is divided into three chapters. Chapter 1, “Perspectives,” presents what the entirely new field of home computer graphics is all about, explains how it got started, and illustrates some of the exciting applications for low-cost graphics displays. Chapter 2, “Basic Concepts,” introduces the general hardware and software concepts behind computer graphics and continues by presenting a profile of the numerous products on the market today. A section on graphics accessories is also included.  Chapter 3, the meat of the book, is entitled “Graphics Programming.” It introduces the graphics features of the Apple II computer used for this book, and then goes on to describe these concepts: plotting simple equations; drawing lines and vectors; creation of simple geometric shapes (rectangles, triangles, polygons, circles) as well as gaming figures (small tanks, jets, cars, rackets, animals); mandalas and other computer art effects, including tunneling; shape shifting, random artwork; detailed drawings and the use of digitizing tables; and, finally, moving figure animation.  The first two chapters of the book can be read any time and will be of help in evaluating which personal computer to buy for graphics work. The third chapter can be studied whether or not you own a computer, but your understanding will certainly be enhanced if one is available to practice the examples on.  The author hopes that you find this journey into computer graphics exciting, comprehensive, and, most of all, enjoyable.","","calma, engineering","",""
10,"","","Veen, Arthur H.","Dataflow machine architecture","",,,"12","",1986,"","","https://www.researchgate.net/publication/220566271","","","","","Surveys, A. C. M.  Computing","","","","","","","","Dataflow machines are programmable computers of which the hardware is optimized for fine-grain data-driven parallel computation. The principles and complications of data-driven execution are explained, as well as the advantages and costs of fine-grain parallelism. A general model for a dataflow machine is presented and the major design options are discussed.Most dataflow machines described in the literature are surveyed on the basis of this model and its associated technology. For general-purpose computing the most promising dataflow machines are those that employ packet-switching communication and support general recursion. Such a recursion mechanism requires an extremely fast mechanism to map a sparsely occupied virtual space to a physical space of realistic size. No solution has yet proved fully satisfactory.A working prototype of one processing element is described in detail. On the basis of experience with this prototype, some of the objections raised against the dataflow approach are discussed. It appears that the overhead due to fine-grain parallelism can be made acceptable by sophisticated compiling and employing special hardware for the storage of data structures. Many computing-intensive programs show sufficient parallelism. In fact, a major problem is to restrain parallelism when machine resources tend to get overloaded. Another issue that requires further investigation is the distribution of computation and data structures over the processing elements.","","dataflow, data-driven computing, architecture","",""
10,"","","Freeman, Ross H.","Configurable electrical circuit having configurable logic elements and configurable interconnects","",,US4870302A,"9","",1989,"","","https://patents.google.com/patent/US4870302A/","","","","","","","","","","","","","A configurable logic array comprises a plurality of configurable logic elements variably interconnected in response to control signals to perform a selected logic function. Each configurable logic element in the array is in itself capable of performing any one of a plurality of logic functions depending upon the control information placed in the configurable logic element. Each configurable logic element can have its function varied even after it is installed in a system by changing the control information placed in that element. Structure is provided for storing control information and providing access to the stored control information to allow each configurable logic element to be properly configured prior to the initiation of operation of the system of which the array is a part. Novel interconnection structures are provided to facilitate the configuring of each logic element.","","fpga, xilinx","",""
10,"","","Kahng, A. B. & Pati, Y. C.","Subwavelength optical lithography, challenges and impact on physical design","",,,"4","",1999,"","","https://vlsicad.ucsd.edu/Publications/Conferences/94/c94.pdf","","","","","of Computer Science, U. C. L. A.  Department & Numerical Technologies, Inc","","","","","","","","We review the implications of subwavelength optical lithography for new tools and ows in the interface between layout design and manufacturability. After discussing the necessity of corrections for optical process effects (i.e., use of optical proximity correction (OPC) and phase-shifting masks (PSM)), we focus on the implications of OPC and PSM for layout and verication methodologies. Our discussion addresses the necessary changes in the design-to- manufacturing ow, including infrastructure development in the mask and process communities as well as opportunities for research and development in physical layout and verification.","","","",""
10,"","","Schüler, E. & Helfers, Tim","XPP - eXtreme Processing Platform Technology for space applications","",,,"9","",2001,"","","http://spacewire.esa.int/WG/Data-Systems/OPDP-proceedings/Presentations/session%20II.B/3_Astrium_XPP_Helfers.pdf","","","","","Astrium, P. A. C. T.","","","","","","","","","","xpp, architecture, von neumann, fpga","",""
10,"","","Zoltan Baruch, Octavian Creţ & Pusztai, Kalman","Configurable processor","",,,"5","",2002,"","","https://www.researchgate.net/publication/229001961_CONFIGURABLE_PROCESSOR","","","","","of Cluj-Napoca, Technical University","","","","","","","","Configurable architectures can deliver the high performance required by computationally-demanding applications, similar to the ASIC circuits, while providing the flexibility of the programmable processors. The performances achieved by these architectures are often one or two orders of magnitude higher than those of processor-based alternatives. In this paper we describe the design and implementation of a configurable processor. The proces- sor consists of a constant part and a configurable structure. The constant part allows to solve simple applications without changing the existing resources. The configurable part is defined by the user, based on the requirements of a specific application. This part contains application- specific functional blocks, controlled by special instructions. The integration of a classical proc- essor and a configurable architecture within the same circuit allows to exploit the advantages of both architectures. For the design of the configurable processor we used the VHDL language. The implementation was performed using a Xilinx XCV600E FPGA device. This processor can be used in several types of applications: data encryption and compression, image processing, digital signal processing, special arithmetic.","","Configurable computing, Configurable architectures, Reconfigurable devices, FPGA devices.","",""
10,"","","Compton, Katherine & Hauck, Scott","Reconfigurable computing: a survey of systems and software","",,,"5","",2002,"","","https://people.ece.uw.edu/hauck/publications/ConfigCompute.pdf","","","","","Surveys, A. C. M.  Computing","","","","","","","","Due to its potential to greatly accelerate a wide variety of applications, reconfigurable computing has become a subject of a great deal of research. Its key feature is the ability to perform computations in hardware to increase performance, while retaining much of the flexibility of a software solution. In this survey, we explore the hardware aspects of reconfigurable computing machines, from single chip architectures to multi-chip systems, including internal structures and external coupling. We also focus on the software that targets these machines, such as compilation tools that map high-leve lalgorithms directly to the reconfigurable substrate. Finally, we consider the issues involved in run-time reconfigurable systems, which reuse the configurable hardware during program execution","","reconfigurable computing, fpga, architecture","",""
10,"","","Gonzalez, Ricardo E.","A Software Configurable Processor","",,,"9","",2005,"","","https://pdfs.semanticscholar.org/89ad/640a6e704844a0098278e79a8ef06a934c62.pdf","","","","","Inc, Stretch","","","","","","","","","","assp, asic, fpga, gpu, dsp, cpu, configurable computing","",""
10,"","","WEBER, Charles; BERGLUND, C. Neil & GABELLA, Patricia","Mask cost and profitability in photomask manufacturing, an empirical analysis","",,,"11","",2006,"","","http://web.pdx.edu/~webercm/documents/2006%20November%20IEEE%20TSM%20Weber%20Berglund%20Gabella.pdf","","","","","University, Portland State","","","","","","","","An empirical study of the economics of manufacturing photomasks concludes that the uncontrolled growth of optical proximity effect correction and resolution enhancement techniques is driving up the cost of pattern generation and mask inspection to levels that threaten the profitability of photomask manufacturing. The intrinsic cost of some leading edge photomasks has already exceeded the price that customers are willing to pay for them. A model of the lifecycle of photomask manufacturing, developed from interviews involving the 1990 to 2005 operations of six mask shops and a survey of seven photomask manufacturers, shows that design for manufacturability (DFM) constitutes the most promising approach for alleviating this market impasse. Unilateral action by mask shops to increase their capital productivity is necessary but insufficient and perhaps unaffordable. DFM solutions will require the majority of participants in the lithography value chain to collaborate according to a volatile demand schedule that is driven by semiconductor manufacturers.","","masks, costs, profitability, photomask, manufacturing","",""
10,"","","Carton, Olivier","Transistors et portes logiques","",,,"9","",2006,"","","https://www.irif.fr/~carton/Enseignement/Architecture/Cours/Gates/","","","","","de Recherche en Informatique Fondamentale, Institut","","","","","","","","","","porte, gate, transistor, not, nand, nor, xor, cmos","",""
10,"","","Prado, Daniel Francisco Gómez","Tutorial on FPGA routing","",,,"8","",2006,"","","http://sisbib.unmsm.edu.pe/bibvirtualdata/publicaciones/electronica/n17_2006/a04.pdf","","","","","of Massachusetts, University","","","","","","","","The   entire   CAD   process   that   is   necessary   to   implement   a   circuit   in   an   FPGA   (from   the   RTL   description  of  the  design)  consists  of  the  following  steps: •Logic  optimization. Performs  two-level  or  multi-level    minimization    of    the    Boolean    equations    to    optimize    area,    delay,    or    a    combination of both.  •Technology     mapping.     Transforms     the     Boolean  equations  into  a  circuit  of  FPGA  logic  blocks.   This   step   also   optimizes   the   total   number     of     logic     blocks     required     (area     optimization)  or  the  number  of  logic  blocks  in  time-critical paths (delay optimization).  •Placement.  Selects  the  specific  location  for  each  logic  block  in  the  FPGA,  while  trying  to  minimize    the    total    length    of    interconnect    required. •Routing.   Connects   the   available   FPGA’s   routing    resources1    with    the    logic    blocks    distributed  inside  the  FPGA  by  the  placement  tool,   carrying   signals   from   where   they   are   generated to where they are used.","","routing, fpga, model","",""
10,"","","Unknown","6502 Schematic","",,,"11","",2007,"","","https://downloads.reactivemicro.com/Electronics/CPU/6502%20Schematic.pdf","","","","","Unknown","","","","","","","","","","6502, schematic","",""
10,"","","","Oral history panel on the development and promotion of the Motorola 68000","",,,"7","",2007,"","","https://archive.computerhistory.org/resources/access/text/2012/04/102658164-05-01-acc.pdf","","","","","Museum, Computer History","","","","","","","","","","68000, motorola","",""
10,"","","Drepper, Ulrich","What every programmer should know about memory, Part 1","",,,"9","",2007,"","","https://lwn.net/Articles/250967/","","","","","LWN","","","","","","","","In the early days computers were much simpler. The various components of a system, such as the CPU, memory, mass storage, and network interfaces, were developed together and, as a result, were quite balanced in their performance. For example, the memory and network interfaces were not (much) faster than the CPU at providing data.  This situation changed once the basic structure of computers stabilized and hardware developers concentrated on optimizing individual subsystems. Suddenly the performance of some components of the computer fell significantly behind and bottlenecks developed. This was especially true for mass storage and memory subsystems which, for cost reasons, improved more slowly relative to other components.  The slowness of mass storage has mostly been dealt with using software techniques: operating systems keep most often used (and most likely to be used) data in main memory, which can be accessed at a rate orders of magnitude faster than the hard disk. Cache storage was added to the storage devices themselves, which requires no changes in the operating system to increase performance. {Changes are needed, however, to guarantee data integrity when using storage device caches.} For the purposes of this paper, we will not go into more details of software optimizations for the mass storage access.","","","",""
10,"","","Weisberg, David E.","The Engineering Design Revolution","",,,"","",2008,"","","http://www.cadhistory.net/11%20CALMA.pdf","","","","","","","","","","","","","","","cad, engineering, calma","",""
10,"","","Wayne Wolf, Ahmed Amine Jerraya & Martin, Grant","Multiprocessor System-on-Chip (MPSoC) Technology","",,,"10","",2008,"","","http://www.cs.unc.edu/~montek/teaching/Comp790-Fall11/Home/Home_files/2008Wolf.pdf","","","","","IEEE","","","","","","","","The  multiprocessor  system-on-chip  (MPSoC)  usesmultiple  CPUs  along  with  other  hardware  subsystems  to  imple-ment a system. A wide range of MPSoC architectures have beendeveloped over the past decade. This paper surveys the history ofMPSoCs to argue that they represent an important and distinctcategory of computer architecture. We consider some of the tech-nological trends that have driven the design of MPSoCs. We alsosurvey computer-aided design problems relevant to the design ofMPSoCs","","Configurable processors, encoding, hardware/software codesign, multiprocessor, multiprocessor system-on-chip(MPSoC)","",""
10,"","","Articles, A. R. M.  Technical Support Knowledge","What is the difference between a von Neumann architecture and a Harvard architecture?","",,,"9","",2008,"","","http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.faqs/ka3839.html","","","","","Limited, A. R. M.","","","","","","","","","","von neumann, harvard, architecture","",""
10,"","","Fouquet-Lapar, Matthias","The von Neumann Architecture and Alternatives","",,,"5","",2008,"","","https://wwz.ifremer.fr/pcdm/content/download/29481/407627/file/mfl.pdf","","","","","SGI","","","","","","","","","","von neumann, bottleneck, xpp, hpc, fpga, gpu, ibm cell","",""
10,"","","","Altera EP300 Design \& Development Oral History Panel","",,,"10","",2009,"","","https://archive.computerhistory.org/resources/access/text/2012/10/102702147-05-01-acc.pdf","","","","","Museum, Computer History","","","","","","","","","","altera, ep300, interview, Source III","",""
10,"","","McGrath, Dylan","FPGA startups stare down giants and ghosts","",,,"7","",2009,"","","https://www.eetimes.com/document.asp?doc_id=1263547","","","","","Times, Electronic Engineering","","","","","","","","","","timeline, fpga, vendors, xilinx, altera","",""
10,"","","Jeff Chase, Brent Nelson, John Bodily Zhaoyi Wei & Lee, Dah-Jye","Real-Time Optical Flow Calculations on FPGA and GPU Architectures: AComparison Study","",,,"4","",2009,"","","https://www.researchgate.net/profile/Lee_Dah-Jye/publication/224362818_Real-Time_Optical_Flow_Calculations_on_FPGA_and_GPU_Architectures_A_Comparison_Study/links/0c9605327135c229e0000000.pdf","","","","","University, Brigham Young","","","","","","","","FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing a variety of computations. Applications successfully implemented on FPGAs have typically contained high levels of parallelism and have often used simple statically-scheduled control and modest arithmetic. Recently introduced computing devices such as coarse grain reconfigurable arrays, multi-core processors, and graphical processing units (GPUs) promise to significantly change the computational landscape for the implementation of high-speed real-time computing tasks. One reason for this is that these architectures take advantage of many of the same application characteristics that fit well on FPGAs. One real-time computing task, optical flow, is difficult to apply in robotic vision applications in practice because of its high computational and data rate requirements, and so is a good candidate for implementation on FPGAs and other custom computing architectures. In this paper, a tensor-based optical flow algorithm is implemented on both an FPGA and a GPU and the two implementations discussed. The two implementations had similar performance, but with the FPGA implementation requiring 12×more development time. Other comparison data for these two technologies is then given for three additional applications taken froma MIMO digital communication system design, providing additional examples of the relative capabilites of these two technologies.","","fpga, gpu, comparison","",""
10,"","","Kidd, Taylor IoT","Why P scales a C*V^2*f is so obvious","",,,"6","",2009,"","","https://software.intel.com/en-us/blogs/2009/06/29/why-p-scales-as-cv2f-is-so-obvious","","","","","Zone, Intel Developer","","","","","","","","","","power, energy, consumption, cmos, physics","",""
10,"","","","Kit d’apprentissage de l’électronique pour débutants","",,,"7","",2009,"","","https://produktinfo.conrad.com/datenblaetter/175000-199999/192230-an-01-fr-LERNPAKET_25_ELEKTRONIK_EXPERIMENTE.pdf","","","","","Conrad","","","","","","","","","","électronique, composants, conrad, circuit","",""
10,"","","Jones, David H.; Powell, Adam; Bouganis, Christos-Savvas & Cheung, Peter Y. K.","GPU versus FPGA for high productivity computing","",,,"8","",2010,"","","http://cas.ee.ic.ac.uk/people/ccb98/papers/DavidFPL10.pdf","","","","","London, Imperial College","","","","","","","","Heterogeneous or co-processor architectures arebecoming an important component of high productivity computing systems (HPCS). In this work the performance of a GPU based HPCS is compared with the performance of a commercially available FPGA based HPC. Contrary to previous approaches that focussed on specific examples, a broader analysis is performed by considering processes at an architectural level. A set of benchmarks is employed that use different process architectures in order to exploit the benefits of each technology. These include the asynchronous pipelines common to “map” tasks, a partially synchronous tree common to “reduce” tasks and a fully synchronous, fully connected mesh. We show that the GPU is more productive than the FPGA architecture for most of the benchmarks and conclude that FPGA-based HPCS is being marginalised by GPUs.","","hpcs, high productivity computing system, architecture, gpu, fpga, comparison","",""
10,"","","Kalarot, Ratheesh & Morris, John","Comparison of FPGA and GPU implementations of Real-time Stereo Vision","",,,"5","",2010,"","","https://www.researchgate.net/profile/John_Morris25/publication/224165460_Comparison_of_FPGA_and_GPU_implementations_of_real-time_stereo_vision/links/0f317539b0c42b50be000000.pdf","","","","","of Auckland / IEEE, The University","","","","","","","","Real-time stereo vision systems have many applications -from autonomous navigation for vehicles through surveillance to materials handling. Accurate scene interpretation depends on an ability to process high resolution images in real-time, but, although the calculations for stereo matching are basically simple, a practical system needs to evaluate at least 10-9 disparities every second - beyond the capability of a single processor. Stereo correspondence algorithms have high degrees of inherent parallelism and are thus good candidates for parallel implementations. In this paper, we compare the performance obtainable with an FPGA and a GPU to understand the trade-off between the flexibility but relatively low speed of an FPGA and the high speed and fixed architecture of the GPU. Our comparison highlights the relative strengths and limitations of the two systems. Our experiments show that, for a range of image sizes, the GPU manages 2×10-9 disparities per second, compared with 2.6×10-9 disparities per second for an FPGA.","","gpu, fpga, comparison","",""
10,"","","Cox, Russ","The MOS 6502 and the Best Layout Guy in the World","",,,"1","",2011,"","","https://research.swtch.com/6502","","","","","","","","","","","","","What are the key designs of the 6502 compared to other processors of its time","","","",""
10,"","","Culver, John","How a CPU Microprocessor is made","",,,"4","",2011,"","","http://www.cpushack.com/MakingWafers.html","","","","","Shack, The C. P. U.","","","","","","","","","","wafer, silicon, fabrication","",""
10,"","","Feugey, David","Altera mise sur l’OpenCL pour révolutionner le monde des FPGA","",,,"11","",2011,"","","https://www.silicon.fr/altera-mise-sur-l%E2%80%99opencl-pour-revolutionner-le-monde-des-fpga-65255.html","","","","","","","","","","","","","","","opencl, cuda, gpu, fpga, altera","",""
10,"","","Johnson, Jeff","Outsourcing FPGA design: pros and cons","",,,"8","",2011,"","","http://www.slideshare.net/jeffjohnsonau/outsourcing-fpga-design-pros-and-cons","","","","","","","","","","","","","manage fluctuating workloads, specialized expertise (learning curve is steep and long, experienced FPGA developers are hard to find), start projects sooner, greater flexibility","","outsourcing, fpga","",""
10,"","","VERRY, Tim","Apple’s A6 processor uses hand drawn ARM cores to boost performance","",,,"9","",2012,"","","https://pcper.com/2012/09/apples-a6-processor-uses-hand-drawn-arm-cores-to-boost-performance/","","","","","Perspective, P. C.","","","","","","","","","","ARM, A15, Apple, A6, iPhone 5,hand drawn","",""
10,"","","Nenni, Daniel","A Brief History of FPGAs","",,,"8","",2012,"","","https://semiwiki.com/fpga/1596-a-brief-history-of-fpgas/","","","","","","","","","","","","","","","xilinx, fpga, history","",""
10,"","","persons, Various","What are some examples of non-Von Neumann architectures?","",,,"11","",2012,"","","https://stackoverflow.com/questions/1806490/what-are-some-examples-of-non-von-neumann-architectures","","","","","StackOverflow","","","","","","","","","","von neumann, architecture, harvard, dataflow, reduction, cellular automata, quantum computing","",""
10,"","","Fowers, Jeremy; Brown, Greg; Cooke, Patrick & Stitt, Greg","A performance and energy comparison of FPGAs, GPUs and Multicores for sliding-window applications","",,,"2","",2012,"","","http://www.gstitt.ece.ufl.edu/courses/spring13/eel4712/lectures/fpga130-fowers.pptx","","","","","of Florida, University","","","","","","","","","","performance, energy, comparison, fpga, gpu, multicore","",""
10,"","","","An FPGA-based supercomputer for statistical physics: the weird case of Janus","",,,"5","",2012,"","","https://www.eweb.unex.es/eweb/fisteor/juan/PUBLICATIONS/janusSubmitted.pdf","","","","","","","","","","","","","","","","",""
10,"","","","P.A. Semi","",,,"","",2013,"","","https://en.wikipedia.org/wiki/P.A._Semi","","","","","Wikipedia","","","","","","","","","","fabless, semiconductor, acquisition, Apple","",""
10,"","","","Electrically Erasable Programmable Logic PEEL 18CV8","",,,"7","",2013,"","","https://www.datasheetarchive.com/pdf/download.php?id=3ae6b7f4f1c26b281f249beac3c15d411ba916&type=O","","","","","Electronics, Gould","","","","","","","","","","","",""
10,"","","","Implementing FPGA design with the OpenCL standard","",,,"11","",2013,"","","https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/wp/wp-01173-opencl.pdf","","","","","Altera","","","","","","","","The initial era of programmable technologies contained two different extremes of programmability. As illustrated in Figure 1, one extreme was represented by single core CPU and digital signal processing (DSP) units. These devices were programmable using software consisting of a list of instructions to be executed. These instructions were created in a manner that was conceptually sequential to the programmer, although an advanced processor could reorder instructions to extract instruction-level parallelism from these sequential programs at run time. In contrast, the other extreme of programmable technology was represented by the FPGA. These devices are programmed by creating configurable hardware circuits, which execute completely in parallel. A designer using an FPGA is essentially creating a massively-fine-grained parallel application. For many years, these extremes coexisted with each type of programmability being applied to different application domains. However, recent trends in technology scaling have favored technologies that are both programmable and parallel.","","opencl, altera, fpga","",""
10,"","","Miller, Warren","Configurable processors as an alternative to FPGAs","",,,"7","",2013,"","","https://www.eetimes.com/author.asp?section_id=36&doc_id=1318804","","","","","Times, Electronic Engineering","","","","","","","","An exploration of using configurable processors as an alternative to the traditional FPGA approach to creating a custom system.","","configurable processor, fpga, isef","",""
10,"","","Higginbotham, Stacey","Why Microsoft is building programmable chips that specialize in search","",,,"6","",2014,"","","https://gigaom.com/2014/06/16/why-microsoft-is-building-programmable-chips-that-specialize-in-search/","","","","","GigaOM","","","","","","","","","","microsoft, fpga, cpu, comparison","",""
10,"","","Hindriksen, Vincent","Why use OpenCL on FPGAs?","",,,"9","",2014,"","","https://streamhpc.com/blog/2014-09-16/use-opencl-fpgas/","","","","","StreamHPC","","","","","","","","Altera has just released the free ebook FPGAs for dummies. One part of the book is devoted to OpenCL, so we’ll quote some extracts here from one of the chapters. The rest of the book is worth a read, so if you want to check the rest of the text, just fill in the form on Altera’s webpage","","altera, fpgas for dummies, opencl","",""
10,"","","VAndrei","Von Neumann vs Harvard architecture","",,,"11","",2014,"","","https://stackoverflow.com/questions/26826248/von-neumann-vs-harvard-architecture","","","","","StackOverflow","","","","","","","","","","von neumann, harvard, architecture","",""
10,"","","Jones, Dr. Handel","Why migration to 20 nm bulk CMOS and 16/14 nm FinFets is not best approach for semiconductor industry","",,,"1","",2014,"","","http://caxapa.ru/thumbs/598000/WP_handel-jones.pdf","","","","","Strategies, International Business","","","","","","","","The growth of the semiconductor industry hashistoricallyhad strong dependenceon the reduction in cost  per  transistor.The  analysisof  the  cost per  gate at  20nm,  however,indicates  that  conventional scaling approaches for bulk CMOS have reached saturation. Asimilar perspective exists for the initial generation of FinFET structuresat 16/14nm where cost per gateis higher than for 20nm and 28nm.High volume applicationsneed lower cost per transistorin order to use the new generation of process technologies.This  pattern  has  been  consistent  since  the  development  of  integrated  circuits.  It  is,consequently, appropriate to evaluate the optionsfor continuing the pattern of lower cost per gate.","","economic, cmos, engraving, semiconductor, cost per transistor","",""
10,"","","McMillan, John","PCB design then and now","",,,"7","",2015,"","","https://blogs.mentor.com/jimmartens/blog/2015/07/14/pcb-design-then-and-now/","","","","","Mentor, a Siemens Business","","","","","","","","","","pcb, cad, calma, digitizing","",""
10,"","","Franz, Kaitlyn","History of the FPGA","",,,"1","",2015,"","","https://blog.digilentinc.com/history-of-the-fpga/","","","","","Inc, Digilent","","","","","","","","","","fpga, timeline, technology","",""
10,"","","","Intel completes acquisition of Altera","",,,"12","",2015,"","","https://newsroom.intel.com/news-releases/intel-completes-acquisition-of-altera/","","","","","Intel","","","","","","","","Intel Corporation (“Intel”) today announced that it has completed the acquisition of Altera Corporation (“Altera”), a leading provider of field-programmable gate array (FPGA) technology. The acquisition complements Intel’s leading-edge product portfolio and enables new classes of products in the high-growth data center and Internet of Things (IoT) market segments.","","","",""
10,"","","Higginbotham, Stacey","Why Intel will spend $16.7 billion on Altera","",,,"8","",2015,"","","http://fortune.com/2015/08/27/why-intel-altera/","","","","","Fortune","","","","","","","","","","acquisition, intel, altera","",""
10,"","","Harris, Derrick","Microsoft is building fast, low-power neural networks with FPGAs","",,,"2","",2015,"","","https://gigaom.com/2015/02/23/microsoft-is-building-fast-low-power-neural-networks-with-fpgas/","","","","","GigaOM","","","","","","","","","","microsoft, ai, fpga, neural network","",""
10,"","","Ovtcharov, Kalin; Ruwase, Olatunji; Kim, Joo-Young; Fowers, Jeremy; Strauss, Karin & Chung, Eric S.","Accelerating deep convolutional neural networks using specialized hardware","",,,"2","",2015,"","","https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN20Whitepaper.pdf","","","","","Research, Microsoft","","","","","","","","Recent breakthroughsin the development of multi-layer convolutional neural networkshave led to state-of-the-art improvements in the accuracy of non-trivial recognition tasks such as large-category image classificationand automaticspeech recognition.These many-layered neural networks are large, complex,and require substantial computingresourcesto train and evaluate. Unfortunately, these demands come at an inopportunemoment due to the recent slowing of gains in commodity processor performance.Hardware specializationin the form of GPGPUs, FPGAs, and ASICs offers a promising path towards major leaps in processing capabilitywhile achieving high energy efficiency. To harness specialization, an effort is underwayat Microsoft to accelerate Deep Convolutional Neural Networks (CNN) usingservers augmented with FPGAs—similar to the hardware that is being integrated intosome of Microsoft’s datacenters. Initial efforts to implement a single-node CNN accelerator on a mid-rangeFPGA showsignificant promise, resulting in respectableperformance relative to prior FPGA designs and high-end GPGPUs, at a fraction of the power. In the future, combining multiple FPGAs over a low-latency communication fabric offers further opportunity to train and evaluate models of unprecedented size and quality","","microsoft, neural network, ai, fpga","",""
10,"","","",""Les grands mythes fondateurs" des nanos : la loi de Moore ou l'héritage du talk de Feynman de 1959","",,,"6","",2015,"","","http://www.unit.eu/cours/enjeux-nanosciences-nanotechnologies/Module3-FR.pdf","","","","","et Technologie), U. N. I. T.  (Université Numérique Ingénierie","","","","","","","","Précisions sur les prévisions de Gordon Earle Moore et la fameuse loi de Moore.","","myth, moore's law, richard feynman, nanotechnology","",""
10,"","","Kidd, Taylor IoT","Why P scales as C*V^2*f is so obvious (pt 2)","",,,"1","",2015,"","","https://software.intel.com/en-us/blogs/2009/08/25/why-p-scales-as-cv2f-is-so-obvious-pt-2-2","","","","","Zone, Intel Developer","","","","","","","","","","power, energy, consumption, cmos, physics","",""
10,"","","","The end of Moore’s law","",,,"4","",2015,"","","https://www.economist.com/the-economist-explains/2015/04/19/the-end-of-moores-law","","","","","Economist, The","","","","","","","","","","moore, future, computing","",""
10,"","","Dettmers, Tim","Is implementing deep learning on FPGAs a natural next step after the success with GPUs?","",,,"4","",2015,"","","https://www.quora.com/Is-implementing-deep-learning-on-FPGAs-a-natural-next-step-after-the-success-with-GPUs","","","","","","","","","","","","","","","deep learning, fpga, gpu","",""
10,"","","","Cilbr8tor Series","",,,"","",2016,"","","https://www.ucamco.com/en/hardware/photoplotters/calibr8tor/calibr8tor-series","","","","","Ucamco, former Barco E. T. S.","","","","","","","","","","photoplotter","",""
10,"","","HKallaher, Brandon","PAL vs. CPLD vs. FPGA","",,,"8","",2016,"","","https://blog.digilentinc.com/pal-vs-cpld-vs-fpga/","","","","","Blog, Digilent","","","","","","","","","","pal, cpld, fpga, comparison, use case","",""
10,"","","Denisenko, Dmitry","OpenCL for FPGAs","",,,"6","",2016,"","","https://cpufpga.files.wordpress.com/2016/04/opencl_for_fpgas_isca_2016.pdf","","","","","Intel","","","","","","","","","","opencl, fpga, altera, intel","",""
10,"","","Eijkhout, Victor","Are there alternatives to the Von Neumann architecture?","",,,"1","",2016,"","","https://www.quora.com/Are-there-alternatives-to-the-Von-Neumann-architecture","","","","","Quora","","","","","","","","","","alternative, von neumann, dataflow","",""
10,"","","Vašut, Marek","Open-Source Tools for FPGA Development","",,,"10","",2016,"","","https://www.youtube.com/watch?v=MI18Wk4gxA4","","","","","Engineering, D. E. N. X.  Software & Foundation, The Linux","","","","","","","","Programmable hardware is becoming increasingly popular in the recent years, yet the software tools for working with such programmable hardware are dominated by closed-source proprietary solutions. This is now changing. In this presentation, Marek will summarize the open-source tools for working with programmable hardware, like "icestorm", "vtr", "ghdl" and "iverilog". Marek will show how to use the open-source tools to produce a working design and explain the benefits and limitations of such solutions. At the end of the talk, Marek will outline the process of implementing such tools to demonstrate why this is so much effort.","","open source, fpga, tools, development","",""
10,"","","","After Moore's law, the future of computing","",,,"3","",2016,"","","https://www.economist.com/leaders/2016/03/12/the-future-of-computing","","","","","Economist, The","","","","","","","","The era of predictable improvement in computer hardware is ending.[…] Making transistors smaller no longer guarantees that they will be cheaper or faster. […] Chips will still get better, but at a slower pace (number-crunching power is now doubling only every 2.5 years, says Intel). The future of computing will be defined by improvements in three other areas, beyond raw hardware performance : software, cloud and new computing architectures. For the technology industry itself, the decline of Moore’s law strengthens the logic for centralised cloud computing, already dominated by a few big firms: Amazon, Google, Microsoft, Alibaba, Baidu and Tencent.","","moore, computing, future","",""
10,"","","","GPU vs FPGA Performance Comparison (white paper)","",,,"5","",2016,"","","http://www.bertendsp.com/pdf/whitepaper/BWP001_GPU_vs_FPGA_Performance_Comparison_v1.0.pdf","","","","","Processing, Berten Digital Signal","","","","","","","","","","comparison, gpu, fpga, white paper","",""
10,"","2019-08-14","","When (and why) is it a good idea to use an FPGA in your embedded system design?","",,,"12","",2017,"","","https://www.embeddedrelated.com/thread/4878/when-and-why-is-it-a-good-idea-to-use-an-fpga-in-your-embedded-system-design","","","","","Related, Embedded","","","","","","","","","","determinism, performance, IO flexibility/complicated interfaces, rapidly evolving standards, security","",""
1,"978-1-119-39049-7","","Moore, Andrew & Wilson, Ron","FPGAs for Dummies","",,,"1","",2017,"","","https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/misc/fpgas_for_dummies_ebook.pdf","","","","","Intel","","","","","","","","Field programmable gate arrays (FPGAs) are integrated cir-cuits that enable designers to program customized digital logic in the field. FPGAs have been around since the 1980s and were originally conceived to give all design teams the ability to create custom logic. In the early days, using an FPGA in your design meant you had to do a lot of programming just to get your FPGA to perform simple functions, so most design-ers avoided them. If you haven’t looked into FPGAs since your university studies way back when, you’ll want to take another look at them.The FPGA has evolved from a useful but humble interface device into a system-level integrated circuit (IC) with its own microprocessors, memory blocks, and interfaces. It’s the next big thing.Now would be a great time to get an inexpensive development kit, download free tools, and begin to explore this world for yourself. And this book will help you understand the practical uses of FPGAs.","","fpga, intel, altera, dummies","",""
10,"","","Staff, I. B. M.  Research Editorial","IBM Scientifs Demonstrate In-memory Computing with 1 Million Devices for Applications in AI","",,,"10","",2017,"","","https://www.ibm.com/blogs/research/2017/10/ibm-scientists-demonstrate-memory-computing-1-million-devices-applications-ai/","","","","","Blog, I. B. M.  Research","","","","","","","","","","in-memory computing, computational memory, ibm, phase change memory","",""
10,"","","Sebastian, Abu; Tuma, Tomas; Papandreou, Nikolaos; Gallo, Manuel Le; Kull, Lukas & Eleftheriou, Thomas Parnell \& Evangelos","Temporal correlation detection using computational phase-change memory","",,,"10","",2017,"","","https://www.nature.com/articles/s41467-017-01481-9","","","","","communications, Nature","","","","","","","","Conventional computers based on the von Neumann architecture perform computation by repeatedly transferring data between their physically separated processing and memory units. As computation becomes increasingly data centric and the scalability limits in terms of performance and power are being reached, alternative computing paradigms with collocated computation and storage are actively being sought. A fascinating such approach is that of computational memory where the physics of nanoscale memory devices are used to perform certain computational tasks within the memory unit in a non-von Neumann manner. We present an experimental demonstration using one million phase change memory devices organized to perform a high-level computational primitive by exploiting the crystallization dynamics. Its result is imprinted in the conductance states of the memory devices. The results of using such a computational memory for processing real-world data sets show that this co-existence of computation and storage at the nanometer scale could enable ultra-dense, low-power, and massively-parallel computing systems.","","phase-change memory","",""
10,"","","Sato, Kaz; Young, Cliff & Patterson, David","An in-depth look at Google’s first Tensor Processing Unit (TPU)","",,,"5","",2017,"","","https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu","","","","","Google","","","","","","","","There’s a common thread that connects Google services such as Google Search, Street View, Google Photos and Google Translate: they all use Google’s Tensor Processing Unit, or TPU, to accelerate their neural network computations behind the scenes.","","tpu, tensor processing unit, google, architecture, reconfigurable computing","",""
10,"","","Dean, Jeff & Hölzle, Urs","Build and train machine learning models on our new Google Cloud TPUs","",,,"5","",2017,"","","https://blog.google/products/google-cloud/google-cloud-offer-tpus-machine-learning/","","","","","Google","","","","","","","","","","tpu, tensor processing unit, google, reconfigurable computing, ai","",""
10,"","","Wood, Lamont","CPU architecture after Moore’s law: what’s next?","",,,"7","",2017,"","","https://www.computerworld.com/article/3209724/cpu-architecture-after-moores-law.html","","","","","Computerworld","","","","","","","","“In five years we will be 10% ahead of where we are now,” [Jim Turley, founder of Silicon Insider] predicts. “Every few years there is a university research project that thinks they are about to overturn the tried-and-true architecture that John von Neumann and Alan Turing would recognize — and unicorns will dance and butterflies will sing. It never really happens, and we just make the same computers go faster and everyone is satisfied. In terms of commercial value, steady, incremental improvement is the way to go.” “Power dissipation is the whole deal,” says Tom Conte, a professor at the Georgia Institute of Technology and past president of the IEEE Computer Society. “Removing 150 watts per square centimeter is the best we can do without resorting to exotic cooling, which costs more.” “Eight things in parallel is about the limit, and hardly any programs use more than three or four cores. So we have run into a wall on getting speed from cores. The cores themselves are not getting much wider than 64 bits. Intel-style cores can do about five instructions at a time, and ARM cores are up to three, but beyond five is the point of diminishing returns, and we need new architecture to get beyond that. The bottom line is traditional software will not get much faster.” − Linley Gwennap, analyst at The Linley Group.","","","",""
10,"","","","Field Programmable Gate Array (FPGA) History and Applications","",,,"2","",2018,"","","http://hardwarebee.com/field-programmable-gate-array-fpga-history-applications/","","","","","Hardwarebee","","","","","","","","","","altera, fpga, history","",""
10,"","","Reese, Lynnette","Comparing hardware for artificial intelligence: FPGAs vs. GPUs vs. ASICs","",,,"7","",2018,"","","https://eecatalog.com/intel/2018/07/24/comparing-hardware-for-artificial-intelligence-fpgas-vs-gpus-vs-asics/","","","","","Solutions, Embedded Intel","","","","","","","","","","fpga, asic, gpu, cpu, comparison","",""
10,"","","","FPGA vs CPU vs GPU vs Microcontroller","",,,"10","",2018,"","","https://static4.arrow.com/-/media/images/research-and-events/articles/1018
/arrow_fpgavscpuvsgpuvsmicrocontroller.pdf","","","","","Arrow","","","","","","","","Tableaux comparatifs des points forts des différentes technologies","","fpga, cpu, gpu, comparison, asic","",""
10,"","","Dubuc, Damien","Afin de terminer notre série de billets, voici quelques réflexions et perspectives que nous ressortons de l’étude","",,,"2","",2018,"","","https://www.aneo.eu/2018/02/06/perspectives-quant-a-lutilisation-fpga-chez-aneo-billet-8/","","","","","Aneo","","","","","","","","","","","",""
10,"","","Castells-Rufas, David","Workshop: programming FPGAs with OpenCL","",,,"5","",2018,"","","http://www.sie.es/wp-content/uploads/2018/06/FPGA-with-OpenCL.pdf","","","","","Cephis","","","","","","","","","","opencl, fpga, energy efficiency","",""
10,"","","Staff, I. B. M.  Research Editorial","IBM Scientists Demonstrate Mixed-Precision In-Memory Computing for the First Time; Hybrid Design for AI Hardware","",,,"4","",2018,"","","https://www.ibm.com/blogs/research/2018/04/ibm-scientists-demonstrate-mixed-precision-in-memory-computing-for-the-first-time-hybrid-design-for-ai-hardware/","","","","","Blog, I. B. M.  Research","","","","","","","","","","in-memory computing, hybrid, ibm","",""
10,"","","Thornton, Scott","What's the difference between Von-Neumann and Harvard architectures?","",,,"3","",2018,"","","https://www.microcontrollertips.com/difference-between-von-neumann-and-harvard-architectures/","","","","","Tips, Microcontroller","","","","","","","","","","architecture, von neumann, harvard, bottleneck","",""
10,"","","Arenas, Aaron","Introduction to FPGA design in Quartus","",,,"8","",2018,"","","https://fpgawiki.intel.com/uploads/0/07/Intro_to_FPGA_Workshop_Slides.pdf","","","","","Intel","","","","","","","","FPGAs at Intel Fundamentals of Digital Electronics FPGA Architecture Intel® Quartus® Prime Design Software FPGA Design Flow","","fpga, quartus, intel","",""
10,"","","Haff, Gordon","What comes after Moore's law","",,,"9","",2018,"","","https://enterprisersproject.com/article/2018/9/what-comes-after-moores-law","","","","","Project, The Enterprisers","","","","","","","","Moore’s Law has also led to a computing environment that tends to be dominated by general-purpose designs. […] Reasonable people may disagree on exactly where we sit in the Moore’s Law endgame, a question that’s increasingly hard to even precisely frame as process nodes like 10nm become more akin to marketing terms than literal descriptions of feature sizes. […] However, no one seriously disputes that Moore’s Law is getting close to fundamental physical limits as processor features are approaching the size of atoms.","","moore, computing, future","",""
10,"","","Gartenberg, Chaim","How Intel’s 9th Gen chips show the way forward after Moore’s Law","",,,"10","",2018,"","","https://www.theverge.com/circuitbreaker/2018/10/11/17963356/intel-9th-gen-chips-moores-law-future-processors-cores","","","","","Verge, The","","","","","","","","Intel’s 9th Gen Core processors are here, and, as expected, they’re simply a refresh of the last generation of chips, with the same 14nm process the company’s been using since 2014. […] Intel still hasn’t managed to move on from its 14nm manufacturing node to the next step, its repeatedly delayed 10nm process.","","intel, next generation, processor","",""
10,"","","Rayome, Alison DeNisco","How programming will change over the next 10 years: 5 predictions","",,,"9","",2018,"","","https://www.techrepublic.com/article/how-programming-will-change-over-the-next-10-years-5-predictions/","","","","","TechRepublic","","","","","","","","1) Programming will be more abstract 2) AI will become part of every developer's toolkit, but won't replace them. Development tools will try to predict developers' intent and make it quicker for them to express that intent, which in the end, becomes another form of abstraction 3) A universal programming language will arise. To reap the benefits of emerging technologies like AI, programming has to be easy to learn and easy to build upon. 4) Every developer will need to work with data. Developers of the future will need to learn more skills, particularly in data analysis. 5) Programming will be a core tenet of the education system.","","programming, future","",""
10,"","","Burt, Jeffrey","FPGA make Xilinx says the future of computing if ACAP","",,,"3","",2018,"","","https://www.nextplatform.com/2018/03/19/fpga-maker-xilinx-says-the-future-of-computing-is-acap/","","","","","Platform, The Next","","","","","","","","","","xilinx, fpga, future, computing","",""
10,"","","","Computers for electronic and mechanical engineering","",,,"","",2019,"","","https://www.museumwaalsdorp.nl/en/history/comphistory/computer-history-the-period-1986-1989/comp866e/","","","","","Waalsdorp, Museum","","","","","","","","","","calma, cad, engineering, pcb","",""
10,"","","","Cyclone V Device Handbook: Volume 1: Device Interfaces and Integration, Logic Array Blocks and Adaptive Logic Modules in Cyclone V Devices","",,,"5","",2019,"","","https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/hb/cyclone-v/cv_5v2.pdf","","","","","Altera, Intel","","","","","","","","","","altera, intel, cyclone v","",""
10,"","","","BitFusion, the elastic AI infrastructure for multi-cloud","",,,"6","",2019,"","","https://bitfusion.io/","","","","","BitFusion","","","","","","","","","","gpu, fpga, asic, ai, performance","",""
10,"","","","Infineon et NXP devant STMicroelectronics au 1er trimestre 2019","",,,"5","",2019,"","","https://www.vipress.net/infineon-et-nxp-devant-stmicroelectronics-au-1er-trimestre-2019/","","","","","ViPress.net","","","","","","","","Après avoir déjà dépassé Samsung au 4e trimestre 2018, Intel renforce ainsi son avance sur le Coréen de près de 3 milliards de dollars au 1er trimestre 2019, selon IC Insights. Intel a détrôné Samsung en tant que premier fournisseur de semiconducteurs au 4e trimestre 2018 après avoir perdu sa place au profit de Samsung au 2e trimestre 2019. Alors que Samsung occupait le premier rang du classement pour l’ensemble de 2017 et de 2018, Intel devrait reprendre facilement le premier rang pour l’année entière de 2019, poste qu’il occupait auparavant de 1993 à 2016. Le retournement des marchés des mémoires Drams et flash NAND au cours de l’année écoulée explique ce basculement. Au premier trimestre 2018, les ventes totales de semiconducteurs de Samsung étaient supérieures de 23% à celles d’Intel ; au 1er trimestre 2019, c’est l’inverse : celles d’Intel dépassent celles du Coréen de 23% !","","semi-conducteur, classement, intel, samsung, ventes","",""
10,"","","","Cyclone V Device Datasheet","",,,"1","",2019,"","","https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/hb/cyclone-v/cv_51002.pdf","","","","","Altera, Intel","","","","","","","","","","cyclone v, fpga, intel, altera, maximum, frequency, performance","",""
10,"","","Cantrill, Bryan","No Moore Left to Give","",,,"6","",2019,"","","https://www.slideshare.net/bcantrill/no-moore-left-to-give-enterprise-computing-after-moores-law","","","","","Joyent","","","","","","","","","","moore's law, computing","",""
10,"","","Suryavansh, Manu","Google Coral Edge TPU Board Vs NVIDIA Jetson Nano Dev board - Hardware comparison","",,,"4","",2019,"","","https://towardsdatascience.com/google-coral-edge-tpu-board-vs-nvidia-jetson-nano-dev-board-hardware-comparison-31660a8bda88","","","","","Science, Towards Data","","","","","","","","Both NVidia and Google recently released dev board targeted towards EdgeAI and also at a cost point to attract developers, makers and hobbyists. Both the dev boards are primarily for inference, but support limited transfer learning re-training. The Edge TPU supports transfer learning training using weight imprinting technique. Both of the dev kits consists of a SOM (System-on-Module) connected to a dev board which has various connectors like USB, Ethernet, microSD slots etc. This is a comparison of the hardware for the two dev kits which can be used as Single board computer (SBC) and not the Edge TPU USB stick. If you don’t want to read the whole article, in my opinion the Coral Edge dev kit is slightly better value for the money as it includes essential peripherals like Wifi and Bluetooth however the Jetson Nano has better software support (both INT8 and FP16 Inference).","","tpu, nvidia, google, coral, jetson","",""
10,"","","Sterckval, Sam","Google Coral Edge TPU vs NVIDIA Jetson Nano : A quick deep dive into Edge AI performance","",,,"4","",2019,"","","https://blog.usejournal.com/google-coral-edge-tpu-vs-nvidia-jetson-nano-a-quick-deep-dive-into-edgeai-performance-bc7860b8d87a","","","","","Noteworthy","","","","","","","","Recently I’ve been reading, testing, and writing a bit about edge computing (like here, and here), with the main focus on edge AI. With cool new hardware hitting the shelves recently, I was eager to compare performance of the new platforms, and even test them against high performance systems.  Of these 3 bars, 2 of them where achieved by the Google Coral Edge TPU USB accelerator, and the 3rd one was a full blown NVIDIA GTX1080 assisted by an Intel i7–7700K. The GTX1080 draws a maximum of 180W, which is absolutely HUGE compared to the Corals 2.5W.  Next thing we see, is that the NVIDIA Jetson Nano isn’t scoring well at all. Although it has a CUDA enabled GPU, it’s really not much faster then my old i7–4870HQ. But that’s the catch, ‘not much faster’, it still is faster then a 50W, quad-core, hyperthreading CPU. From a few years back, true, but still. The Jetson Nano never could have consumed more then a short term average of 12.5W, because that’s what I’m powering it with. That’s a 75% power reduction, with a 10% performance increase.","","jetson, nvidia, google, coral, tpu","",""
10,"","","","Comparison of two new machine learning accelerators, Coral and Jetson Nano","",,,"5","",2019,"","","https://blog.grusbv.com/comparison-of-google-coral-and-nvidia-jetson-nano/","","","","","Blog, Grus","","","","","","","","NVidia favors flexibility with its multiple GPU cores to efficiency, which Google favors with its stripped down core with only 8-bit integer support. In the few benchmarks that were made available by NVidia, Google excels in application performance, except with the Inception-v4 model. With its tiny size, Coral surprises many, and it is their selling point. In short, it seems that Nvidia favors upgradability, development flexibility whereas Google favors the performance and connectivity.  I would say that in order to begin working on machine learning on embedded systems, (or as some call AI on the edge) I would recommend the Jetson Nano. Though there is a sense of urgency around the topic and a powerful impetus towards inference performance, development environment still counts. Switching, if necessary would not be much of an issue.  We, on the other hand, are designing on both of them, actually also with many others including Intel Movidius Neural Compute Stick or straight STM32 ARM Cortex microcontrollers with machine learning optimizations. They cover a wide range of the ML inference domain, and should be chosen specifically for the application.","","google, nvidia, jetson, coral, tpu","",""
10,"","","Feldman, Michael","With Agilex, Intel gets a coherent FPGA strategy","",,,"4","",2019,"","","https://www.nextplatform.com/2019/04/02/with-agilex-intel-gets-a-coherent-fpga-strategy/","","","","","Platform, The Next","","","","","","","","","","intel, agilex","",""
